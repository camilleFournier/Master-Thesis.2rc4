\begin{document}

\chapter{Introduction}

One of the biggest challenge of mobile development today resides in the high fragmentation of mobile platforms \cite{MobileDevChallenges} with developers building the same app over multiple platforms. A popular solution is cross-platform development, that is using a framework capable of using the same code for multiple platform builds. However, such cross-platform development does not concern web applications that developers still have to develop separately from mobile applications. To answer this problem, Google introduced in 2015 the concept of \textit{Progressive Web Apps} \cite{PWA_intro}, a term first coined by designer Frances Berriman and Google Chrome  developer Alex Russel \cite{PWA_blog} \cite{PWApossibleUnifer}.
This section will introduce the concept of Progressive Web Apps and the objective of this paper.


\section{Research questions}

Due to its cross-platform capabilities, Progressive Web Apps seems like a promising alternative to native development. However, this shouldn't come at the cost of app performance and user experience, especially in regards to smoothness.
Thus, this paper will aim at answering the question : 
\begin{center}
    \textit{Can Progressive Web Apps be as smooth as Mobile Applications ?}
\end{center}
Which raises the following sub-questions: 
\begin{enumerate}
    \item What metrics are relevant to compare the smoothness of Mobile and Progressive Web Apps?
    \item What tools are available to measure those metrics ?
\end{enumerate}

\iffalse
Due to its cross-platform capabilities, Progressive Web Apps seems like a promising alternative to native development. However, this shouldn't come at the cost of app performance and user experience, especially in regards to smoothness.
Thus, this paper will aim at answering the question : 
\begin{center}
    \textit{Are Progressive Web Apps as performant as Mobile Applications in terms of smoothness?}
\end{center}
Which can be divided into the sub-questions: 
\begin{enumerate}
    \item How can we compare the smoothness of Progressive Web Apps and Mobile Applications ?
    \item With the metrics identified previously, are Progressive Web Apps as smooth as Mobile Applications ?
    \item How many resources are used to render a smooth application ?
\end{enumerate}
\fi

\iffalse
Primary research question : 
\newline \textit{How efficient can Progressive Web Apps be compared to a Native Android Application?}
\newline
Secondary Research questions :
\\1. \textit{How can we compare Progressive Web Apps and Mobile Apps ?}
\newline \indent
1.1 \textit{What metrics are relevant to compare Progressive Web Apps and Mobile Apps ?}
\newline \indent  
1.2 \textit{What tools are available to measure those metrics?}
\newline
\\2. \textit{Are Progressive Web Apps more efficient than Mobile Apps with regards to the first question?}
\fi

Due to its cross-platform capability, Progressive Web Apps seems like a promising alternative to native development. However, this shouldn't come at the cost of app performance and user experience, especially in terms of the app smoothness and rendering capacity.
Thus, this paper will aim at answering the question : 
\begin{center}
    \textit{Are Progressive Web Apps more performant than Mobile Apps in terms of rendering capacity ?}
\end{center}
Which can be divided into the following sub-questions: 
\begin{enumerate}
    \item How can we compare Progressive Web Apps with Mobile Applications?
    \item What tools are available to measure the relevant metrics ?
\end{enumerate}

\chapter{Background}
\section{Mobile Applications}
\subsection{Development}

React Native, Ionic and PhoneGap are one of the most popular frameworks used today \cite{CrossPlatform_study}.

\section{Progressive Web Apps}
\subsection{Academic research}

Since June 2019, more than half of Web traffic comes from mobile devices. \cite{WebTrafficData}

    \cite{smoothnessQoE} build a model to evaluate the smoothness of smartphones. This model focuses on two major indexes : the maximum frame interval and the total number of frames in different key behavior scenarios (phone calls, browsing the web, playing games, etc). Those frames are defined as the interval of time between user input and response time of several key display events measured with a camera. %To check this frame definition
    
    \cite{PWAbc_responsetime} measured the response time between an Android app and a React PWA when accessing the Camera and Maps. The response time was measured 54 times for each scenario. The results show that Native are faster for accessing the camera, though PWAs are making progress in that regard. However, PWAs are faster regarding geolocation and map rendering.
    
    \cite{YbergViktor2018NPaU} focused on the user experienced of PWAs and measured several performance metrics: load times (Polymer, React, Preact), start-up times (native app, Polymer with and without browser in memory), Lighthouse performance audit (Polymer), list rendering times (PWA). He concluded that PWAs provided a good enough user experience for information access apps, though PWAs are not fully supported on every platform and web browser.
    
    \cite{PWApossibleUnifer}, followed by \cite{Biorn-Hansen2} shows a lack of academic research on the subject. \cite{PWApossibleUnifer} developped three apps using the Hybrid, Interpreted and PWA approach and compared their startup speed, app size and rendering speed. They conclude that PWA have a lot of potential to unify web and mobile development, though some limitations remains (lacking some hardware access, and browser support on some platform).
    
    \cite{Biorn-Hansen2} developed 5 apps: a PWA, a native app written in Kotlin, an Ionic hybrid app, an Interpreted React Native app and a cross-compiled Xamarin app. Again, they measure the app size, the activity launch time and the time from app icon to toolbar rendering. They reconfirmed the potential of PWAs to have an important part in futur Mobile development, depending on iOS support. They are extremely small compared to other apps, and render content fast. Although they lack some hardware access, they are also the only ones testable before install. 
    
    \cite{SW_and_energy} studied the energy impact of service workers on PWAs. Their results showed no significant energy impact from service workers. However, it also showed different energy consumption of each PWAs
    
    \cite{JohannsenFabian2018PWAa} studied how turning a regular web app into a PWA affects the code complexity in Angular. It concluded that the main increasing factor is the understanding of the new concepts behind the service workers, but automated PWA tooling can decrease it.
    
    \cite{Pride_Prejudice} identified several security threats regarding PWAs
    
    \cite{bac_pwa_comparison} found that PWAs had a slower performance than regular web apps (with lighthouse performance metrics)
    
    \cite{PWA_UX_comparison_study} conducted an analysis of user experience when interacting with a regular web app, a native Android app and a PWA. The 8 participants had an overall good user experience on every platform.
    
    \cite{bac_pwa_performance} studied loading performance (first paint, etc) of PWA in comparison to Apache Cordova app and native android. The results differ between phones %Conclusion noot very clear on this one
    
    \cite{emulating_native_w_crossplatform} compared features from a cross-platform app, a native Android app and a PWA. They were also compared in a qualitative and a qualitative user studies. No real difference was found between the applications. The React Native app performed a little better on the quantitative study, and the PWA performed a little better on the qualitative study.
    
    \cite{PWADatabase} compare performance of same app developed multiple times with different parameters (framework, service-worker, optimization, etc)
    
    \cite{PWAapplicability} compared the same app developed 4 times : a) as a native iOS, b) a native Android, c) a web app and d) a pwa in regards to First Paint and Energy consumption. Regarding the First Paint metric, the PWA performed better against the web app and the native Android app. The difference with iOS might be because of the incomplete support of PWA on iOS. As for energy consumption, PWA seemed to consume less energy, but the number of experiments was not enough to really confirm that.

\section{Profiling tools}

In order to answer the answer the research question, several metrics will be measured across a number of experiments. The most important one is information regarding the frames produced by the application
%, with a target rate of 60 frame per second \citationneeded \todo{Why?}
. Other metrics will be measured to assess the rendering capacity of the applications : the \%CPU usage of the application as well as the memory used by the GPU. This section covers the tools available to automatically collect the data needed as well as the tools chosen for this study.  


\subsection{Android}
However, this tool is not suited for automatic data collection as the results are only available in charts. Moreover, it can have a high overhead \cite{nanoscope} making the collected data less precise.

\paragraph{}
\textbf{Android Debug Bridge} \cite{adb} is a command-line tool used to communicate with an Android device (physical or emulator) from a computer. It can be used to list available devices (command \textit{devices}), forward requests (command \textit{forward} or execute commands on the device (command \textit{shell}). It will be widely used during this study. 

\paragraph{}
\textbf{Dumpsys} \cite{dumpsys} used with \textit{adb shell} provides information about the device's system services. A lot of different services (thus information) is available through this command-line tool, for example the network, the battery usage, input events, \red{etc}. The most interesting services for this study are: \todo{Rephrase} 
\begin{itemize}
    \item \textit{gfxinfo} which provides information regarding the animation such as jank rate (percentage of frames considered janky ie not meeting the 60fps threshold), total number of frames since recording, etc. It can be used with two options : \textit{framestats} which gives additional information about the frame timestamps, and \textit{reset} which reset the information recorded until now. This information is retrieved by Android framework during the application run-time.
    
    \item \textit{cpuinfo} gives the \%CPU usage of each process running during a given interval of time. The system computes this information from the proc/stat/ files (similar to the files of the same name found on Linux systems) and displays it as the percentage of CPU time used over the total CPU time available in one CPU core with a 0.1\% precision. The information is updated whenever the system needs it and  \todo[color=cyan!20]{Do I cite the source code for that ?} \todo{Yes:)} at least every 30min so the output is not always up-to-date.  
    
    \item \textit{meminfo} takes a snapshot of the memory allocation and outputs the data in details. The main field of interested displays by this command is the amount of memory used for graphics buffer queues. Coincidentally, it also checks the information contained by \textit{cpuinfo} is up-to-date. Thus, it is also used in the experiments to update \textit{cpuinfo}
\end{itemize}

\paragraph{}
\textbf{Top} comes from the Linux command-line of the same name, but with limited options. It displays the process activity in real-time (\%CPU, memory used and other). Its source of information is the same as dumpsys. However, the \%CPU displayed is the percentage of CPU time used over the total CPU time available in the device, with a 1\% precision. This makes it less accurate especially for processes that doesn't consume a lot of CPU. Thus, this command-line was discarded in favor of dumpsys.

\paragraph{}
\textbf{Systrace} \cite{systrace} records a device activity during a short period of time and outputs it in the form of an html file. The file can be viewed on Chrome Browser and displays a timeline of events for each threads running during the recording. The events displayed depends on the categories selected when starting the recording. This tool isn't suitable for automated experiments but can be really helpful to understand the cause of a bottleneck or, as it is the case in this study, understand how Android system works.

\paragraph{}
\textbf{Systrace} \cite{systrace} records a device activity during a short period of time and outputs it in the form of an html file. The file can be viewed on Chrome Browser and displays a timeline of events for each threads running during the recording. The events displayed depends on the categories selected when starting the recording. This tool isn't suitable for automated experiments but can be really helpful to understand the cause of a bottleneck or, as it is the case in this study, understand how Android system works.

\paragraph{}
\textbf{Monkeyrunner} \cite{monkeyrunner} is a tool used to interact with an Android device from a Python script. Aside from sending events and executing \textit{adb shell} commands, it can also take screenshots to test the app. This tool was used to automate the experiment and remove the variables introduced by human interaction from the experiments. 

\subsection{Chrome}

As Progressive Web Apps are after all web applications that runs on the browser, many of the tools available depends on the browser running the app. Thus, the tools presented here may only apply to apps running on Chrome.

\paragraph{}
\textbf{Chrome DevTools} is a set of developer tools available in the Chrome Browser in order to inspect a page on the browser (the network, the performance, the memory, the elements, etc). The main tool of interest here is the \textit{Performance} panel \cite{chrome_devtools_perf} which can record certain events during a period of time and display a lot of information from these events. Among the features of interest are : the display of the duration of a frame as well as the CPU time used to compute it, a CPU, an FPS and a memory chart in addition to a timeline of different events recorded.

\paragraph{}
\textbf{Lighthouse} \cite{lighthouse} is an automated  tool used to assess any web page's quality but mainly targeting Progressive Web Apps \cite{PWApossibleUnifer}. It is available from the \textit{Audits} panel on Chrome Devtools and can retrieve a large number of metrics from an automated recording. Those metrics are classified into five different groups: Performance, Accessibility, Best Practices, Search Engine Optomization (SEO) and Progressive Web Apps, and give an overall score from 0 to 100. It was often used in previous research on PWA \cite{}

\section{Rendering pipeline}

\subsection{Android}
Multiple Application Renderers can communicate with SurfaceFlinger at the same time.   When an application needs to display something on the screen, it pushes a 'surface' to a BufferQueue shared with SurfaceFlinger. Every once in a while, usually in synchronization with the screen refresh rate, SurfaceFlinger wakes up and look into the BufferQueues.

\subsection{Chrome}
However, some stages can be skipped depending on the change between two frames.

\section{Problem analysis}
In the light of the background described above, this sections provides additional motivation and limitations for the research questions. 
\paragraph{}
\textbf{Motivation} \newline

Previous research about Progressive Web Apps performance focused mainly on the performance at start-up (loading time, first paint) but none was found that studied the rendering performance during use. To my best knowledge, this research will be the first to do so.

\medskip
\textbf{Limitations} \newline
As stated above, Android applications and Web Applications on Chrome share two components of the display pipeline (SurfaceFlinger and the Hardware Composer) but differ greatly in the Application Renderer. Thus this research will focus on comparing the smoothness of Android applications and Progressive Web Apps in regards to this last component. 

Thus, this paper will analyze smoothness performance of the applications by looking at the average frame duration, as well as CPU and memory used to compare the resources used. 

\iffalse
However, at the time of writing no study was found that examined the rendering performance of Progressive Web Apps. While it is important for the user that the app launches quickly \cite{launch_time}, it is also important that the app remains smooth during the time of use. \newline
Thus, this paper will try to address this issue and study the smoothness of mobile applications and progressive web apps.

Most of the time, the smoothness of an application is associated with its responsiveness i.e. how quickly it can respond to user input.

As defined by nanana, "the principle of smoothness states that objects must change in a continuous fashion, which reduces cognitive load by removing large and unexpected changes in visual information presented to the user". In other words, a smooth animation changes frames and with little changes between them 

In this work, the smoothness of an application is defined as how quickly it can change the display

\paragraph{}
The smoothness of a mobile application is usually evaluated using the frame rate, or FPS for frame rate per second \cite{smoothnessQoE} \todo[inline]{It would be good to clarify what is smoothness and fluidness}. Though as Biørn-Hansen et al. pointed out while studying animation performance in mobile applications, the FPS metric does not give much insight about the fluidness of a user interface where animations run only during a short amount of time and the remaining time is spent idle waiting for new inputs.
\newline
Thus, this paper will analyze the smoothness performance of the applications by looking at the average frame duration, as well as CPU and memory used to compare the resources used. 
\fi

\chapter{Methodology}

\iffalse
Do android tools work on PWA? (after all, there is a webAPK installed when installing the PWA)?
    - cpuinfo : 3 processes shown -> but is there any other work done by Browser besides PWA ? Is there any other way to have \%CPU for PWA and be sure it is only that?
        -> Chrome devtools performance panel cpu graph: how do they compute it ? (amount of time spent inside functions (between start and end time) but doesn't take into account time spent waiting for results or idle -> not accurate enough
        -> cpu time metric computed with each frame (not used for cpu graph as cputime metric isn't computed in remote debugging but still display of the graph)
        -> CDP SystemInfo -> doesn't work actually. Similar issue filed in 2018 on CDP github repo
        -> Chrome devtools CPU sampling (v8.profiler thing)
        
        => computed \%CPU as measured with dumpsys cpuinfo, cputime from chome devtools and from cpu sampling from chrome devtools on a single-core emulated device : chrome devtools methods not accurate enough compared to dumpsys -> thus use dumpsys in future experiments
        
        
    - meminfo (graphics ie  memory used by graphics data buffers) : works
    
    - gfxinfo : doesn't work on web apps. Use of systrace confirms it. We need another way to measure frames on chrome. 
        -> Chrome devtools fps : end of a frame is when another starts and only on Renderer process. Since Android frames ends with a swapping of buffers and this is done on GPU Process on Chrome, need another way to detect them
        -> Chrome inspect tracing tool, visual representation of CDP Tracing experimental domain,  similar to Android Systrace but for web apps on Chrome. Build a model by trial and error. Now can have lots of timestamps regarding Chrome frames. How can they be compared to android frames ? Looked at systrace and the timestamps available with gfxinfo framestats. Matched the timestamps with Systrace by looking at source code
        
        Conclusion about what can be compared (cf weekly report 11)
\fi

\section{Comparing the Rendering Process}
   
    The background section presented an overview of Android's Garphics pipeline, with the Application Renderer itself divided into 7 stages : input handling, animation, measurement and layout, draw, sync and upload, issue commands, process and swap buffers. 
    
    It is possible to extract detailed information about an Android application frames thanks to the command \textit{adb shell dumpsys gfxinfo framestats}. This outputs multiple graphic information, such as total number of frames rendered, number of janky frames
    
    We can observe some of those stages in a Systrace recorded with the command: \textit{python systrace.py sched gfx view app -a [package-name] -o [output-file]}.
    
    \begin{figure}[!ht]
        \includegraphics[width=15cm]{kththesis/Figures/android_systrace_frame.jpg}
        \caption{Application Renderer on Android}
        \label{fig:android_systrace}
    \end{figure}
    
    
    \subsection{Chrome's pipeline}
    \todo[inline, color=cyan!20]{Describe Chrome's pipeline model and how I came up with it}
       As was described in the Background chapter, Chrome contains several processes, including the Browser Process, the Renderer and the GPU Process.
       
       To automate tracing, the Chrome Devtools Protocol was used. Its experimental domain 'Tracing' allows to record the same events as Chrome's Tracing tool. This enable the automation of both tracing and event processing to extract a model. \newline
The resulting model is presented in Figure \ref{fig:chrome_graphics_simple}. For a more detailed version with all the events tracked to know if a frame is truly rendered, see Annex \ref{?}.

To link the empirical model to the pipeline stages presented in the background, Parsing, Styling, Layout, Compositing, Pre-painting, and Painting are all computed for a 'Main frame' in the main thread of the Renderer process CrRendererMain. Tiling and Raster are also computed for a 'Main frame' but in the Compositor thread, sometimes with the help of other threads and even the GPU Process. The last step, Drawing is executed in several steps. The Compositor thread first computes a 'CompositorFrame' from the 'Main frame' available and sends to the VizCompositorThread in the GPU Process. The VizCompositorThread wait for all necessary surfaces and aggregates them, before sending them to the main thread of the GPU Process. CrGpuMain can then swap the buffers to push the frame to SurfaceFlinger. The frame is now complete.

\item The VizCompositorThread, probably because of a VSYNC event that happened previously, asks for a new frame to the Renderer process with the event 'IssueBeginframe' which contains a bind\_id.
    \item The Compositor thread of the Renderer process discard or accept it with the event 'ReceiveBeginFrame' which contains the same bind\_id. This event also call 'Scheduler::BeginFrame' which contains a sequence\_number necessary to follow the frame afterwards.
    \item The Compositor may ask a new main frame, and schedules the next step depending on the urgency of the frame with the event Scheduler::BeginImplFrame containing the same sequence\_number. The event asking for a new main frame contains a begin\_frame\_id useful to follow the frame in the thread CrRendererMain.
    \item If a new main frame was asked, it is compiled in CrRendererMain. The pipeline stages Styling, Layout, Compositing, pre-Painting and Painting are computed here if necessary.
    \item If a new main frame was committed by CrRendererMain, Tiling and Raster may be computed by the Compositor and other threads if necessary. Then, it is activated: the new main frame replaces the old one in the Compositor.
    \item The deadline of the scheduler is up: Scheduler::OnBeginImplFrameDeadline is called with the same sequence\_number as the beginning. This event itself calls several other events : GenerateRenderPass, GenerateCompositorFrame and SendCompositorFrame containing a bind\_id and LayerTreeHost::PrepareToDraw containing a SourceFrameNumber referencing the main frame used for this frame. SendCompositorFrame is not always detected but is an important event to track.
    \item The VizCompositorThread receives the CompositorFrame from the Renderer, and maybe wait for more. It then aggregates the surfaces ie the CompositorFrames into a single frame. It also assigns a put\_offset to the frame. It is not an id per say as it is not unique but the reuse of numbers are spaced enough to use it as IDs.
    \item The main thread of the GPU receives the frame and swap buffers for SurfaceFlinger. Since it is 
    
    \subsection{Method of comparison}
    \todo[inline, color=cyan!20]{Conclusions on how to compare both pipelines: a) definition of a frame b) benchmark apps used}
    
    \iffalse
    When a new surface is available, the VizCompositor thread combines it with the other surface and produces a new frame. The GPU main thread receives it and push it to SurfaceFlinger.
\paragraph{}
To summarize, a frame timeline is as follows:
\begin{enumerate}
    \item The \badge{VizCompositorThread} asks for a new frame to the Renderer process (cf \textit{IssueBeginFrame}).
    \item The Compositor thread of the Renderer process agrees to compute a new frame (cf \textit{ReceiveBeginFrame}) and schedules it (cf \textit{Scheduled}).
    \item The Compositor might also ask for a new main frame (cf \textit{SendRequestMainFrame}). If so, the main thread of the Renderer process CrRendererMain compiles it (cf \textit{BeginMainFrame}). In regards to the pipeline stages presented in the background, it computes the Styling, Layout, Compositing, Pre-Painting and Painting. When CrRendererMain is finished compiling the main frame, it commits it to the Compositor (cf \textit{BeginMainFrameCommit}).
    \item When the Compositor receives a new main frame (cf \textit{BeginCommit}), it computes Tiling and Raster if necessary with the help of other threads. The main frame can then be activated (cf \textit{ActivateLayerTree}): it replaces the old main frame in the memory of the Compositor thread and can be used as a baseline for future frames.
    \item Once the deadline scheduled in step 2 is up, the frame is drawn. This step is the last stage of pipeline presented previously: Drawing. The Compositor generates a ComositorFrame (cf \textit{GenerateRenderPass} and \textit{GenerateCompositorFrame}) with a main frame as a baseline and sends it to the GPU Process (cf \textit{SubmitCompositorFrame}).
    \item The VizCompositorThread receives all necessary surfaces (cf \textit{ReceiveCompositorFrame}), aggregates them into a single frame (cf \textit{SurfaceAggregation}) and sends it to CrGpuMain thread.
    \item Finally, CrGpuMain receives the frame and swap buffers for SurfaceFlinger (cf \textit{SwapBuffers} and \textit{FrameCompleted})
\end{enumerate}


A Main Frame starts to compute the frame at \textit{BeginMainFrame} timestamp. A Basic Frame which re-uses a previous main frame only starts computing at the beginning of 'ProxyImpl::ScheduleActionDraw' event. 

    To summarize, on one hand Android's pipeline is pretty straightforward and easy to follow : the OS sends VSync signals that depends on the device refresh rate, asking everything on the foreground to render a frame. In response, the UI thread calls Choreographer\#doFrame which handles callbacks for input and animation before doing some sizing and layout. The Choreographer then hands what it computed to the RenderThread which issues the draw commands and swap the buffers for SurfaceFlinger.


On the other hand, Chrome's pipeline is much less consistent and depends on a lot of parameters. We know that the VizCompositorThread also calls the Choreographer almost at each Vsync though it stops after the animation callbacks. As it is also the thread emitting 'IssueBeginFrame' events, those events are most likely linked, though I don't know when this IssueBeginFrame event is emitted compared to the Choreographer.

Thus, it is meaningless to compare Chrome's frames with Android's from the start of Choreographer or the start of Vsync.

Another point against doing this is the the input events. Those are always handled on Android inside the Choreographer, just before computing the frame. On Chrome, however, they are handled asynchronously and thus an input can affect a frame currently being drawn or start a new one.
\iffalse
Moreover, the input events also pose a problem. Those are always handled on Android inside the Choreographer, just before computing the frame. However they are handled asynchronously on Chrome and can even be handled in the middle of the computation of a new frame. An input event can affect a frame currently being drawn or start a new one. With this method of comparison, some of the frames on Chrome would contain input events while others would not. Thus, it is irrelevant. 
\fi
This leaves us with comparing the amount of time they spent on computing the frame. For Android, it starts with traversal, and can be accessed with the PerformTraversalStart timestamp. For Chrome, it is a little more complicated as a 'frame' for Android, that is a graphical buffer swapped for SurfaceFlinger, consists of several frames computed in parallel and combined on Chrome's Pipeline. \newline

As a Browser frame on PWA represents things usually handled by the application on Android like the scrollbar or refresh animation, and it also leads to a BufferSwap, 
For a 'Main frame', the computation starts with the \textit{BeginMainFrame} timestamp. For a 'Basic frame' which uses an old main frame as a baseline, it starts with the event 'ProxyImpl::ScheduleActionDraw'. 

For a Basic frame which reuse a Main frame already drawn, I believe the computation starts inside ProxyImpl::ScheduleActionDraw. For a basic frame with a new Main frame, it should start at 'BeginMainThreadFrame' event. 

As a Buffer swap consists of both a Basic frame and a Browser frame, and a Browser frame can trigger a Buffer swap by itself, the question is do we count them as part of the PWA's frame or not. If, as I suppose, the Browser frame displays things like the scrollbar, I believe we should count them as a PWA's frame as these displays are usually handled by the app on Android. 

Thus, the most meaningful comparison, in my opinion, is the time between the timestamps PerformTraversalStart->FrameCompleted on Android and PrepareToDraw/BeginMainThreadFrame(if new main frame)->FrameCompleted of the Compositor frame of the Browser frame on Chrome (depending which took longest for the corresponding Buffer swap).

\iffalse
\begin{enumerate}
    \item When do Chrome starts computing a Main Frame ? \newline
    As the first stages of Chrome's pipeline happens just after \textit{BeginMainFrame}, this timestamp will be considered the start of the computation for a Main Frame.
    \item When do Chrome starts computing a Basic Frame that re-uses a Main Frame ? \newline
    The only stage of Chrome's pipeline computed in a Basic Frame is Drawing, which starts at 'ProxyImpl::ScheduleActionDraw' event. Thus, it will be considered the start of the computation of a Basic Frame.
    \item Do frames which only changes the Browser surface count as frames to compare to Android application ? \newline
    The Browser surface represents on PWA what is usually also handled by the application on Android (scrollbar, refresh animation). Thus, those frames will also be compared to Android frames. 
    \item When both the Renderer surface and the Browser surface changes, what to consider the start of the frame ? \newline
    Since both Browser frames and Renderer frames are to be compared to Android, and they are pushed to SurfaceFlinger as a single frame, the start of the frame will be the start of the Browser or the Renderer frame depending on which happens the earliest. 
\end{enumerate}
\fi

\fi
    
\section{Comparing CPU and Memory}
    \subsection{Android and Chrome tools}
    \todo[inline, color=cyan!20]{Describe available tools on Android and Chrome}
    \todo[inline]{Move to Background}
    \subsection{Evaluating Chrome tools}
    \todo[inline, color=cyan!20]{Describe Experiments used to compare Android and Chrome tools, and their results}
    \subsection{Evaluating Emulators}
    \todo[inline, color=cyan!20]{Explain why Emulators are no good for evaluating CPU and Memory}
\section{benchmark and protocol}

\iffalse
The smoothness metric defined previously depends in Progressive Web Apps on the rendering path taken by the frame. Thus, the benchmark application will have 2 features : one favouring Main Frames and the other favouring Basic Frames. The easiest way to trigger a Main Frame is to change the content of the screen. This can be done with a single click. A Basic Frame is basely triggered  
\fi
\subsection{script implementation}
\iffalse
We will focus on the fluidity of the apps that can be defined with Frame Per Second (FPS) rate.
Thus, we will compare the FPS rate of a Native Android Application and a Progressive Web App, as well as with a benchmark consisting of : 
    - an app displaying different image/texts every 16/17ms
    - an app with more effort on the CPU : modification of the same pictures/texts or new pictures/texts generated randomly.

\newline
\fi

We will measure FPS, CPU and Memory usage to see the impact of CPU and Memory usage in the fluidity of the app.
\newline
We will focus on the fluidity of the apps that can be defined with Frame Per Second (FPS) rate.
Thus, we will compare the FPS rate of a Native Android Application and a Progressive Web App, as well as with a benchmark consisting of : 
    - an app displaying different image/texts every 16/17ms
    - an app with more effort on the CPU : modification of the same pictures/texts or new pictures/texts generated randomly.
\newline

We will measure FPS, CPU and Memory usage to see the impact of CPU and Memory usage in the fluidity of the app.
\newline

\subsection{CPU}

\todo[inline, color=cyan!20]{Remove entirely the next paragraph ?}

\todo[inline]{Well it seems a result of your observations, I think it should not be here after all.}
However, there is a small issue with cpuinfo: the output is not always up-to-date, with sometimes a 15 min difference between the data displayed and the time it is called (it will display 'CPU usage from X ms to X ms ago'). This makes it difficult to use for automated experiments. Upon inspecting the source code\footnote{https://github.com/aosp-mirror/platform\_frameworks\_base/blob/master/services/core/java/com/android/server/am/ActivityManagerService.java} of meminfo and cpuinfo, it was found that the data saved by cpuinfo is updated each time meminfo is called. It was observed that was not always the case when 2 consecutive calls are too close to each other, but it is enough to have CPU usage from at least the last minute.

Upon inspecting Chrome Devtools and its source code\footnote{https://github.com/ChromeDevTools/devtools-frontend}, 3 possible methods of measuring CPU usage were identified:
\begin{enumerate}
    \item CPU graph displayed by Chrome Devtools Performance Panel
    \item cpuTime metric computed by Chrome Devtools for each frame
    \item CPU sampling events saved during a recording
\end{enumerate}

\paragraph{}
According to the source code of Chrome Devtools, the CPU graph is actually computed from the selftime of the functions. The selftime of the function is the amount of time it is running itself and does not wait for a function it called. Though a function usually use the CPU during its chole self-time, that is not always the case. The function can be paused for the CPU to perform other tasks for example. Thus, the CPU graph from Chrome Devtools, though a good approximation, is not accurate enough to compare the CPU usage of Android and Progressive Web applications. \newline
The cpuTime metric provided for each frame is computed similarly but its computation is limited to events related to the computation of the frame. Thus, it is also not usable as an accurate measure.

\paragraph{}
This leaves the CPU sampling events saved during a recording. CPU sampling \cite{cpu_sampling} can evaluate the CPU usage by retrieving regularly the functions currently running on the CPU. The more samples taken, the more accurate the estimation will be. This method was evaluated against the command line \textit{adb shell dumpsys cpuinfo} by measuring the CPU usage of a  Progressive Web App running on an Android emulated device with both methods (CPU sampling and dumpsys).
\color{blue}
The emulated device only had a single CPU core in case the CPU sampling only happen on the CPU cores running the app. The Progressive Web App was able to simulate different CPU workload depending on the frequency of a text changing.
\paragraph{}

\begin{figure}
    \centering
    \includegraphics[width=13cm]{kththesis/Figures/cpu_graph.JPG}
    \caption{CPU usage}
    \label{fig:cpu_usage}
\end{figure}

The results are the average of 100 measures were taken for each method and workload(see \autoref{fig:cpu_usage}). If the CPU sampling from Chrome Devtools was accurate enough, its curve would be close to the measures taken by dumpsys, though always behind as \textit{dumpsys cpuinfo} provides a maximum CPU usage of the application. This is not the case here.

\paragraph{}
Thus, the CPU usage of both Android and Progressive Web application will be measured with the command line \textit{adb shell dumpsys cpuinfo}.

\color{black}
    


\subsection{Benchmark applications}
\paragraph{}
Since a frame in a Progressive Web App can take 2 different rendering path depending on the changes occurring in the frame, at least 2 different features favouring one path or another will have to be developed. The Main Frame path is always triggered when changing some content on the screen since at least Painting is required. The Basic Frame path is favoured when scrolling a page, as the Compositor thread usually only needs to change the position of an already-painted layer against the display screen. 

The frame duration will be evaluated with a 3s recording, the memory snapshot will be executed after 10s of running a scenario in order to update the data for \textit{cpuinfo} and the CPU usage will be extracted shortly after.

\subsection{Scripts implementation}
\todo[inline]{Explain your technical contributions}
In order to remove the human interaction variable from the results, all experiments will be executed by automated scripts. All of them are available in a Git repository\footnote{TODO:git link, maybe create a clean repo with everything after}.

\chapter{Results}
\section{Smoothness performance}
\paragraph{Frame duration}
Though the Progressive Web App is never the fastest one to compute a frame, it is not always the slowest and surpasses the Native application in 3 instances: when changing a picture on Samsung S5 and Huawei P9-Lite, and when changing a text on Samsung S5.

\paragraph{Resources}

The results presented here are the average of 100 measures. In every scenario, the Native application uses less resources than the Hybrid and the PWA, both in terms of CPU and Memory usage. The performance of the Native, Hybrid and Progressive Web App differ depending on the device. \paragraph{}
In Scenario 1 (\autoref{tab:text:changing}), the Progressive Web App takes longer than the Hybrid app to compute a frame with an increase of 36\% on Samsung S6, 11\% on Samsung S5 and 139\% on Huawei. The difference is smaller between the Native application and the Progressive Web App, the latter being even faster than the Native app on Samsung S6.
%However, the Native application consumes a lot less resources than the Hybrid and the Progressive Web app, with the progressive Web App consuming slightly less CPU than the Hybrid app.

\paragraph{}
In Scenario 2 (\autoref{tab:text:scrolling}), the Native application is faster than both the Hybrid and the PWA on every device. A clear difference of performance is visible between the performance on Samsung S6 and the performance on Samsung S5 and Huawei. All apps compute a frame in 17-19ms on Samsung S6 while the Hybrid and Native app take only 3 to 7 ms on Samsung S5 and Huawei. However, the performance of the Progressive Web app does not improve this much and stays between 14 - 17 ms. 
%The Native app consumes less resources than the Hybrid and Progressive Web App, with the Hybrid consuming slightly less CPU than the Progressive Web App this time.

\paragraph{}
In Scenario 3 (\autoref{tab:picture:changing}), the Hybrid application is faster on all three devices. The Progressive Web App is faster than the Native app on two devices, Samsung S5 and Huawei, but lags behind by 20\% on Samsung S6. 
\end{document}

\chapter{Discussion}

This may be due to the size of the frameworks used for the Hybrid and PWA, while the framework used in Native application is part of the OS and not the application.

However, it should be noted that the scrolling experiments were not as consistent between devices and frameworks than when changing content. The same drag event did not trigger the same scrolling speed across device and framework, and the length of the scrolling page was different. It is also difficult to simulate the same drag event with \textit{monkeyrunner} and the Input Domain of Chrome DevTools Protocol. More experiments would be needed to confirm the performance observed when scrolling. Moreover, the clocks of different threads in Chrome were found to be slightly out of sync. Thus, some events were detected before their triggering events on another thread. However, those incidents concerned events not counted in the frame duration. They were scarce and of small magnitude and thus do not invalidate the results presented previously.

\chapter{Conclusion}

One possibility of future work identified in the limitations is the extension of this thesis to other browsers and OS, especially iOS which is often discarded in academic research about Progressive Web Apps. This study can also be extended with other cross-platform and Web framework, for example Xamarin and Angular.\newline
The other possibilities involve studying another aspect of performance of a mobile application, for example its responsiveness, its battery consumption or the performance of other animations like videos or those found in games.
